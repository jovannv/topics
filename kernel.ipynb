{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9f0d9e1da1d1c9ce96322ecedc81d7a7847f043",
    "_cell_guid": "b370038e-edd0-42b6-985a-c30f6eea960b"
   },
   "source": [
    "# NIPS: Topic modeling visualization\n\nSome main topics at NIPS according to [wikipedia](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems) :\n\n1. Machine learning, \n2. Statistics, \n3. Artificial intelligence, \n4. Computational neuroscience\n\nHowever, the topics are within the same domain which makes it more challenging to distinguish between them. Here in this Kernel I will try to extract some topics using Latent Dirichlet allocation __LDA__. This tutorial features an end-to-end  natural language processing pipeline, starting with raw data and running through preparing, modeling, visualizing the paper. We'll touch on the following points\n\n\n1. Topic modeling with **LDA**\n1. Visualizing topic models with **pyLDAvis**\n1. Visualizing LDA results with **t-SNE** and **bokeh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49cf47103ed18adfb84e28e636784cf89dbb3144",
    "collapsed": true,
    "_cell_guid": "a614ee7b-dc9a-4a69-a22a-6712176d67c2",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eda4ba077f4beb7a55182bc37b147050f444827f",
    "collapsed": true,
    "_cell_guid": "9b886989-75f5-4522-b9d6-2a40737885f0",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "p_df = pd.read_csv('../input/Papers.csv')\n",
    "docs = array(p_df['PaperText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "365398b9b4072a8c5bf3c8de0067122e70fff748",
    "_cell_guid": "7364c06e-8044-4da4-9b50-4daa5bf5765e"
   },
   "source": [
    "## Pre-process and vectorize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f10f622826eaa4e9bd97c1414aac5538ddabafd2",
    "collapsed": true,
    "_cell_guid": "125f9eb3-99d9-422a-8242-384da62d43dd",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "790063fc1a341eb17e0244c220de736d51196cfe",
    "collapsed": true,
    "_cell_guid": "c87ebee3-e88b-4233-9000-b71103f281c3",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "docs = docs_preprocessor(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f649c357b776963b7e626064fa6ced1a03a3219d",
    "_cell_guid": "b167c49c-6187-4921-9d2b-2fa1ba8377a4"
   },
   "source": [
    "### **Compute bigrams/trigrams:**\nSine topics are very similar what would make distinguish them are phrases rather than single/individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6ce91b0d51d55e3504cbb352f0548cffcdb0169",
    "collapsed": true,
    "_cell_guid": "66b3e9bf-1c76-4ee1-bb22-60405308c401",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c75a2e62b9a0e570a0a34b84af5912c22aa9ba28",
    "_cell_guid": "b9849dae-b405-4bc6-ab61-c062f2a13746"
   },
   "source": [
    "### **Remove rare and common tokens:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73f7ba9fedcde10c4e70cf2456c7ef20c8e32d35",
    "collapsed": true,
    "_cell_guid": "df440d67-082a-4b2d-a6fb-80029af194b5",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5e4e52fe4ff798ac96f8f23538770c5cc4d217a",
    "_cell_guid": "ac2ede6e-8815-4da1-a739-1ffc71614cb9"
   },
   "source": [
    "Pruning the common and rare words, we end up with only about 6% of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78cd228789c6fa5d094886b58dce875f63625c33",
    "_cell_guid": "0e6a6363-b43f-45f0-80d9-0f5b5b9462fc"
   },
   "source": [
    "** Vectorize data:**  \nThe first step is to get a back-of-words representation of each doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27e71950ec4bca8f6b62ec6d6cd9f613a50019ea",
    "collapsed": true,
    "_cell_guid": "ca1d1e18-2939-4880-bca7-712d4bc42735",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80752c24b3b1bab16b48054a6ca6e7fb84d49372",
    "collapsed": true,
    "_cell_guid": "ad5717c6-b2e8-4122-8c7d-b5bca9dfb1ab",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7f095fc6b213d727b36798294d56a6ab950f5b",
    "_cell_guid": "a0a1a95d-8ed7-47a1-a1b0-0471a77e9af0"
   },
   "source": [
    "With the bag-of-words corpus, we can move on to learn our topic model from the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49a8f93233d2640b476e093390182e21102ae555",
    "_cell_guid": "ff8fbb06-d7f2-4170-b91c-151e3c5c9cad"
   },
   "source": [
    "# Train LDA model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b11f495c4fcf6991c523d5a3b6669506f39533ee",
    "collapsed": true,
    "_cell_guid": "f15e826a-f41b-410a-9ff9-ef19edc05085",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "084c1198f0e528c52edc746436d797c855de8c51",
    "collapsed": true,
    "_cell_guid": "5f8f0fa0-b911-4043-af2c-ec311384d7a5",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 4\n",
    "chunksize = 500 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 400\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5f7c3e213b2fdffa3c767ebd5a1eb7631e76b13",
    "_cell_guid": "93191a68-99d9-4b7d-88b2-5b75e3f44b1d"
   },
   "source": [
    " # How to choose the number of topics? \n__LDA__ is an unsupervised technique, meaning that we don't know prior to running the model how many topics exits in our corpus. Topic coherence, is one of the main techniques used to deestimate the number of topics. You can read about it [here.](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)\n\nHowever, I used the LDA visualization tool **pyLDAvis**, tried a few number of topics and compared the resuls. Four seemed to be the optimal number of topics that would seperate  topics the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e100266faefe67e6ce1131fa464c78630293984",
    "collapsed": true,
    "_cell_guid": "8b9015bc-0e3d-4ee9-9d79-92f2d9dba89f",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f195b367d556a559c4db98446e9c9148758a95f6",
    "collapsed": true,
    "_cell_guid": "483a506d-d21b-4219-948d-87ae641b04ab",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "864ddc392f72ba631bf372d038385d72301816ec",
    "_cell_guid": "aa6b35ff-22bf-44a7-ba48-2c7a368407fe"
   },
   "source": [
    "** What do we see here? **\n\n**The left panel**, labeld Intertopic Distance Map, circles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther.\nThe relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\nAn individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n \n**The right panel**, include the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\nSelecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. \nRelevence is defined as in footer 2 and can be tuned by parameter $\\lambda$, smaller $\\lambda$ gives higher weight to the term's distinctiveness while larger $\\lambda$s corresponds to probablity of the term occurance per topics. \n\nTherefore, to get a better sense of terms per topic we'll use  $\\lambda$=0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "132463131ee73972307d5a3560da8db816f519e2",
    "_cell_guid": "66d42d65-8258-40bf-9fb0-d304f4ed54ea"
   },
   "source": [
    "**How to evaluate our model?**  \nSo again since there is no ground through here, we have to be creative in defining ways to evaluate. I do this in two steps:\n\n1. divide each document in two parts and see if the topics assign to them are simialr. => the more similar the better\n2. compare randomly chosen docs with each other. => the less similar the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71d492523a3cbcfd89141a9d13b66ac5a1f1f2e0",
    "collapsed": true,
    "_cell_guid": "b522a831-eb7e-4d54-a839-7d21d2194a96",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "p_df['tokenz'] = docs\n",
    "\n",
    "docs1 = p_df['tokenz'].apply(lambda l: l[:int0(len(l)/2)])\n",
    "docs2 = p_df['tokenz'].apply(lambda l: l[int0(len(l)/2):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60006b777f680005368652ec6a395d873b80b246",
    "_cell_guid": "b757e081-e199-4f92-8f86-22bd82050dc6"
   },
   "source": [
    "Transform the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e989bc95613b95e5d9c40c0f91f31b93b6c5aab",
    "collapsed": true,
    "_cell_guid": "c88c3646-2a4c-4e77-a6bf-e92ecdcbf627",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "corpus1 = [dictionary.doc2bow(doc) for doc in docs1]\n",
    "corpus2 = [dictionary.doc2bow(doc) for doc in docs2]\n",
    "\n",
    "# Using the corpus LDA model tranformation\n",
    "lda_corpus1 = model[corpus1]\n",
    "lda_corpus2 = model[corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eea1742d65603d9463bc8689a501ac3fda4049c8",
    "collapsed": true,
    "_cell_guid": "d94655b5-1fb1-49ab-af80-90140319a040",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def get_doc_topic_dist(model, corpus, kwords=False):\n",
    "    \n",
    "    '''\n",
    "    LDA transformation, for each doc only returns topics with non-zero weight\n",
    "    This function makes a matrix transformation of docs in the topic space.\n",
    "    '''\n",
    "    top_dist =[]\n",
    "    keys = []\n",
    "\n",
    "    for d in corpus:\n",
    "        tmp = {i:0 for i in range(num_topics)}\n",
    "        tmp.update(dict(model[d]))\n",
    "        vals = list(OrderedDict(tmp).values())\n",
    "        top_dist += [array(vals)]\n",
    "        if kwords:\n",
    "            keys += [array(vals).argmax()]\n",
    "\n",
    "    return array(top_dist), keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36f95ae8a010e1dfd3f441be37b179973c742b1a",
    "collapsed": true,
    "_cell_guid": "48443091-a545-47b3-87c5-69e27985d05c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "top_dist1, _ = get_doc_topic_dist(model, lda_corpus1)\n",
    "top_dist2, _ = get_doc_topic_dist(model, lda_corpus2)\n",
    "\n",
    "print(\"Intra similarity: cosine similarity for corresponding parts of a doc(higher is better):\")\n",
    "print(mean([cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0] for c1,c2 in zip(top_dist1, top_dist2)]))\n",
    "\n",
    "random_pairs = np.random.randint(0, len(p_df['PaperText']), size=(400, 2))\n",
    "\n",
    "print(\"Inter similarity: cosine similarity between random parts (lower is better):\")\n",
    "print(np.mean([cosine_similarity(top_dist1[i[0]].reshape(1, -1), top_dist2[i[1]].reshape(1, -1)) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d69f784d5b4a966efb9c7169938b312e53f25d6c",
    "_cell_guid": "91108906-bd63-4c7b-8b9d-fea0dc5776ce"
   },
   "source": [
    "## Let's look at the terms that appear more in each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d5c2efcf69d5a7a8b4c15d747c3fac732fa1fd7",
    "collapsed": true,
    "_cell_guid": "521793ab-fd0d-4b31-a19c-238482821dfd",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e61600b519fa785ac652a70782a23b229b28be51",
    "collapsed": true,
    "_cell_guid": "8b32274f-ca80-4bd7-88d1-41d207bd2bdd",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "for i in range(num_topics):\n",
    "    print('Topic '+str(i)+' |---------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33ecddaa5755866ad6e9f8f7c5072859d09c27b4",
    "_cell_guid": "2bc62349-d01d-4fe7-8daf-7996155e8c70"
   },
   "source": [
    "From above, it's possible to inspect each topic and assign a human-interpretable label to it. Here I labeled them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a52935b629c3b73bfb9c76536bbc00826fbb8dbe",
    "collapsed": true,
    "_cell_guid": "76ffa777-cce7-46c8-b0fc-3762f514793d",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "top_labels = {0: 'Statistics', 1:'Numerical Analysis', 2:'Online Learning', 3:'Deep Learning'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a88e101807551ee914967e17d542204378db86a",
    "collapsed": true,
    "_cell_guid": "b8b9599c-c6d8-4d7a-a85e-3ec0aae9e951",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def paper_to_wordlist( paper, remove_stopwords=True ):\n",
    "    '''\n",
    "        Function converts text to a sequence of words,\n",
    "        Returns a list of words.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # 1. Remove non-letters\n",
    "    paper_text = re.sub(\"[^a-zA-Z]\",\" \", paper)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = paper_text.lower().split()\n",
    "    # 3. Remove stop words\n",
    "    words = [w for w in words if not w in stops]\n",
    "    # 4. Remove short words\n",
    "    words = [t for t in words if len(t) > 2]\n",
    "    # 5. lemmatizing\n",
    "    words = [nltk.stem.WordNetLemmatizer().lemmatize(t) for t in words]\n",
    "\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3830dfd544633111cfa445aaf44260b2f0a03b32",
    "collapsed": true,
    "_cell_guid": "96d23559-83cd-41cd-9c5c-33bd55afe656",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\\\n",
    "                                  tokenizer=paper_to_wordlist, ngram_range=(1, 3), min_df=40, max_df=0.20,\\\n",
    "                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "dtm = tvectorizer.fit_transform(p_df['PaperText']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "917fb65ed246a61182d9606c6b8cd786abaef53b",
    "collapsed": true,
    "_cell_guid": "630c13fe-57c6-49df-a2f5-2958b70a8271",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "top_dist =[]\n",
    "for d in corpus:\n",
    "    tmp = {i:0 for i in range(num_topics)}\n",
    "    tmp.update(dict(model[d]))\n",
    "    vals = list(OrderedDict(tmp).values())\n",
    "    top_dist += [array(vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7422b187c28d1f2b6da57536a866a25fe9255e24",
    "collapsed": true,
    "_cell_guid": "8d2615ec-62b5-4443-b57c-39482093f7f8",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "top_dist, lda_keys= get_doc_topic_dist(model, corpus, True)\n",
    "features = tvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "389811cde90d4989b10b4fd18d641f9e1896b746",
    "collapsed": true,
    "_cell_guid": "154bfbfe-1c83-4a27-9602-90fbc8c09741",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "top_ws = []\n",
    "for n in range(len(dtm)):\n",
    "    inds = int0(argsort(dtm[n])[::-1][:4])\n",
    "    tmp = [features[i] for i in inds]\n",
    "    \n",
    "    top_ws += [' '.join(tmp)]\n",
    "    \n",
    "p_df['Text_Rep'] = pd.DataFrame(top_ws)\n",
    "p_df['clusters'] = pd.DataFrame(lda_keys)\n",
    "p_df['clusters'].fillna(10, inplace=True)\n",
    "\n",
    "cluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n",
    "\n",
    "p_df['colors'] = p_df['clusters'].apply(lambda l: cluster_colors[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d16af6cfc14d9b8d7d3ac9851e2d4d3ceefe0cc1",
    "collapsed": true,
    "_cell_guid": "a356fc0e-e589-4e5d-b5da-f03328c9dc27",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(top_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e23799bea222f5acb56add0c481c1b0c82fed8b",
    "collapsed": true,
    "_cell_guid": "b62decb3-60e7-4e20-8319-a3fbda8ee1bc",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "p_df['X_tsne'] =X_tsne[:, 0]\n",
    "p_df['Y_tsne'] =X_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f1d4ff6071ed132a4a6a68e86987e65f8bd5956",
    "collapsed": true,
    "_cell_guid": "cc8deea2-d651-4eb9-8928-28882e091c3c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook, save#, output_file\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ddd00b2085860a52118d995cf7c461885cefb4",
    "collapsed": true,
    "_cell_guid": "153dde79-a07b-4041-81ce-e0d2cf4c2f24",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "source = ColumnDataSource(dict(\n",
    "    x=p_df['X_tsne'],\n",
    "    y=p_df['Y_tsne'],\n",
    "    color=p_df['colors'],\n",
    "    label=p_df['clusters'].apply(lambda l: top_labels[l]),\n",
    "#     msize= p_df['marker_size'],\n",
    "    topic_key= p_df['clusters'],\n",
    "    title= p_df[u'Title'],\n",
    "    content = p_df['Text_Rep']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecb9b9f46894e857788141b271be186fb8ffcb2e",
    "collapsed": true,
    "_cell_guid": "2c2e7018-1fc5-4f85-a205-2ae00be40845",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "title = 'T-SNE visualization of topics'\n",
    "\n",
    "plot_lda = figure(plot_width=1000, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x='x', y='y', legend='label', source=source,\n",
    "                 color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"Title: @title, KeyWords: @content - Topic: @topic_key \"}\n",
    "plot_lda.legend.location = \"top_left\"\n",
    "\n",
    "show(plot_lda)\n",
    "\n",
    "#save the plot\n",
    "# save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efd826374b1d516515234bb00c36c9a6f8c997d3",
    "collapsed": true,
    "_cell_guid": "5f7bd5ed-11d1-499b-a7d2-9701b266e694"
   },
   "source": [
    "**What's next?**  \nGiven this topics, one may ask, which topic has been more studied or refered to since NIPS15? For this I extracted and included the citaiton informatin of NIPS'15 papers. I wasn't You can find it at [my github account](https://github.com/ykhorram/nips2015_topic_network_analysis/blob/master/NIP15_topics_citations.ipynb). \n\nPlease let me know if you have any questions or comment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
